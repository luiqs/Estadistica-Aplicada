Modelos de regresión Lineal - Simple y múltiple
================

# 1. Modelo de regresión lineal simple

## A. Creando el modelo de regresión lineal

En un experimento donde se quería estudiar la asociación entre consumo
de sal y presión arterial, se asignó aleatoriamente a algunos individuos
una cantidad diaria constante de sal en su dieta, y al cabo de un mes se
les midio la presión arterial media. Los resultados fueron:

``` r
library(tidyverse)
```

    ## -- Attaching packages --------------------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.3     v purrr   0.3.4
    ## v tibble  3.0.0     v dplyr   1.0.3
    ## v tidyr   1.0.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ------------------------------------------------------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
Presión.Arterial <- tibble( sal = c(1.8, 2.2, 3.5, 4.0, 4.3, 5.0),
                            presión = c(100,98,110, 110, 112, 120))
```

Como clases previas, para la creación de modelos, tenemos funciones
especializadas, las cuales generalmente estan ya dentro de nuetro
paquete básico de “stats”. Como es el caso de la función lm(), para
crear modelos de regresión lineal. Para utilizar **lm()** en nuestro
problema:

``` r
lm(Presión.Arterial$presión ~ Presión.Arterial$sal)
```

    ## 
    ## Call:
    ## lm(formula = Presión.Arterial$presión ~ Presión.Arterial$sal)
    ## 
    ## Coefficients:
    ##          (Intercept)  Presión.Arterial$sal  
    ##               86.371                 6.335

En los resultados, veremos que hemos hayado el intercepto (valor de
alfa).

Para visualizar los detalles del modelo, podemos optar por la función
**summary()**, pero antes, asignemosle un nombre a nuestro modelo.

``` r
Modelo.Lineal.Simple <- lm(data=Presión.Arterial, formula = presión ~ sal )
```

``` r
summary(Modelo.Lineal.Simple)
```

    ## 
    ## Call:
    ## lm(formula = presión ~ sal, data = Presión.Arterial)
    ## 
    ## Residuals:
    ##      1      2      3      4      5      6 
    ##  2.226 -2.309  1.455 -1.712 -1.613  1.952 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  86.3708     3.0621  28.206  9.4e-06 ***
    ## sal           6.3354     0.8395   7.546  0.00165 ** 
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 2.332 on 4 degrees of freedom
    ## Multiple R-squared:  0.9344, Adjusted R-squared:  0.918 
    ## F-statistic: 56.95 on 1 and 4 DF,  p-value: 0.001652

De esta manera podemos ver de manera general alguno de los detalles de
nuestro modelo. Por el momento, observemos que el coeficiente de
determinación se puede observar en la parte inferior y tiene un valor de
0.9344.

## B. El cuarteto de **anscombe**: Cuando el R2 no es suficiente.

Carguemos la base de datos **anscombe**

``` r
data("anscombe")
```

Hallemos la relacion entre cada xi con su yi, y calculemos su R2:

``` r
summary(lm(y1~x1, data = anscombe))$r.squared
```

    ## [1] 0.6665425

``` r
summary(lm(y2~x2, data = anscombe))$r.squared
```

    ## [1] 0.666242

``` r
summary(lm(y3~x3, data = anscombe))$r.squared
```

    ## [1] 0.666324

``` r
summary(lm(y4~x4, data = anscombe))$r.squared
```

    ## [1] 0.6667073

Nos daremos cuenta que los valores del coeficiente de correlación son
muy similares. Pero, que pasa ahora si evaluamos lo que serian sus
gráficas:

``` r
par(mfrow =c(2,2))
plot(y1~x1, data = anscombe)
abline(lm(y1~x1, data = anscombe), col=2)
plot(y2~x2, data = anscombe)
abline(lm(y2~x2, data = anscombe), col=2)
plot(y3~x3, data = anscombe)
abline(lm(y3~x3, data = anscombe), col=2)
plot(y4~x4, data = anscombe)
abline(lm(y4~x4, data = anscombe), col=2)
```

![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Evalue y discuta los que observa. Y responda si es suficiente el R2 para
saber si nuestro modelo lineal es el correcto.

## C. Intervalos de confianza

Para hallar los intervalos de confianza de los estimadores de una
regresion lineal en R, utilizaremos la siguiente función **confint()**
de la siguiente manera:

``` r
confint(Modelo.Lineal.Simple, level = 0.95)
```

    ##                 2.5 %    97.5 %
    ## (Intercept) 77.869064 94.872509
    ## sal          4.004434  8.666266

Para hallar el intervalo de confianza de un determinado valor de X (sal
= 4.5):

``` r
predict(object = Modelo.Lineal.Simple, newdata = data.frame(sal = c(4.5)),
        interval = "confidence", level = 0.95)
```

    ##        fit      lwr      upr
    ## 1 114.8799 111.3041 118.4556

Para hallar el intervalo de predicción de un determinado valor de X (sal
= 4.5):

``` r
predict(object = Modelo.Lineal.Simple, newdata = data.frame(sal = c(4.5)),
        interval = "prediction", level = 0.95)
```

    ##        fit      lwr      upr
    ## 1 114.8799 107.4843 122.2754

## D. Contraste de hipótesis sobre la pendiente de la recta Beta

Para analizar el contraste de hipotesis, podemos evaluar los intervalos
de confianza o podemos evaluar los resultados de p-valor en el resumen
de nuestro modelo:

``` r
summary(Modelo.Lineal.Simple)
```

    ## 
    ## Call:
    ## lm(formula = presión ~ sal, data = Presión.Arterial)
    ## 
    ## Residuals:
    ##      1      2      3      4      5      6 
    ##  2.226 -2.309  1.455 -1.712 -1.613  1.952 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  86.3708     3.0621  28.206  9.4e-06 ***
    ## sal           6.3354     0.8395   7.546  0.00165 ** 
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 2.332 on 4 degrees of freedom
    ## Multiple R-squared:  0.9344, Adjusted R-squared:  0.918 
    ## F-statistic: 56.95 on 1 and 4 DF,  p-value: 0.001652

Los valores Pr(&gt;\|t\|), son los p-value, y nos indican en ambos casos
(aunque solo en B1 tiene sentido) que se rechaza Ho (es decir, que B1 es
diferente de 0).

## E. Representación gráfica del modelo lineal:

La creación de un modelo de regresión lineal simple suele acompañarse de
una representación gráfica superponiendo las observaciones con el
modelo. Además de ayudar a la interpretación, es el primer paso para
identificar posibles violaciones de las condiciones de la regresión
lineal.

Una manera simple de realizarlo con el paquete ggplot2, seria:

``` r
Presión.Arterial %>% ggplot(aes(x=sal, y=presión))+
  geom_point()+
  geom_smooth(method="lm")
```

    ## `geom_smooth()` using formula 'y ~ x'

![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

Sin embargo, no solo el gráfico de la recta nos ayuda a interpretar
nuestro modelo. Tambien podemos visualizar la distribución de los
errores o residuos del modelo. Estos se almacenan como *residuals* en
nuestro modelo. Podemos gráficar la distribucion de los residuos,
mediante:

``` r
plot(Modelo.Lineal.Simple)
```

![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-13-2.png)<!-- -->![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-13-3.png)<!-- -->![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-13-4.png)<!-- -->

Los residuos confirman que los datos no se distribuyen de forma lineal,
ni su varianza constante (plot1). Además se observa que la distribución
de los residuos se aleja de la normalidad debido a algunos de los puntos
(plot2). Solo nos concentraremos en evaluar el plot 1 y 2. Todo este
analisis reduce en gran medida la robustez de la estimación del error
estándar de los coeficientes de correlación estimados y con ello la del
modelo es su conjunto.

## F. Ejercicios de regresión lineal simple:

##### 1. Para este ejercicio, trabajaremos con la base de datos “Boston” del paquete MASS. Aqui la descripción de las variables:

-   crim: ratio de criminalidad per cápita de cada ciudad.
-   zn: Proporción de zonas residenciales con edificaciones de más de
    25.000 pies cuadrados.
-   indus: proporción de zona industrializada.
-   chas: Si hay río en la ciudad (= 1 si hay río; 0 no hay).
-   nox: Concentración de óxidos de nitrógeno (partes per 10 millón).
-   rm: promedio de habitaciones por vivienda.
-   age: Proporción de viviendas ocupadas por el propietario construidas
    antes de 1940.
-   dis: Media ponderada de la distancias a cinco centros de empleo de
    Boston.
-   rad: Índice de accesibilidad a las autopistas radiales.
-   tax: Tasa de impuesto a la propiedad en unidades de $10,000.
-   ptratio: ratio de alumnos/profesor por ciudad.
-   black: 1000(Bk - 0.63)^2 donde Bk es la proporción de gente de color
    por ciudad.
-   lstat: porcentaje de población en condición de pobreza.
-   medv: Valor mediano de las casas ocupadas por el dueño en unidades
    de $1000s.

Realice lo siguiente:

1.  Analice la base y luego realice un modelo de regresión para predecir
    el valor de la vivienda en función del porcentaje de la población.
    Empleando la funcion **lm()** se generará un modelo por minimos
    cuadrados donde la variable respuesta sera *medv* y el predictor
    *lstat*.

2.  Evalue el modelo creado con las funciones: **names()** y
    **summary()**. ¿Qué valores de importancia encuentra y como los
    interpreta?

3.  Calcule usted el los intervalos de confianza para los estimadores
    con la función **confint()** a un nivel de confianza del 95%.

4.  Prediga el valor de la vivienda sabiendo el estatus de la población
    en la que se encuetra. Toda predicción tiene asociado un error y por
    lo tanto un intervalo. Analice e interprete los resultados.

5.  Gráficar la relacion de las variables mediante una regresión lineal.
    Adicionalmente analizar los residuos del modelo.

# 2. Analisis de Regresión Lineal Múltiple

Para el analisis de la regresión lineal múltiple, utilizaremos la base
de datos “softdrink” del paquete “MPV”

``` r
library(MPV)
```

    ## Loading required package: lattice

    ## Loading required package: KernSmooth

    ## KernSmooth 2.23 loaded
    ## Copyright M. P. Wand 1997-2009

``` r
data("softdrink")
```

En el ejemplo los autores ajustaron un modelo de regresión lineal
múltiple para explicar el Tiempo necesario para que un trabajador haga
el mantenimiento y surta una máquina dispensadora de refrescos en
función de las variables Número de Cajas y Distancia.

![](Figuras/RegresionMultiple1.PNG)

Antes de trabajar con la data, vamos a modificar su nombre, para tener
un mejor manejo:

``` r
library(tidyverse)
softdrink <- softdrink %>% rename(tiempo = y, cantidad = x1, distancia = x2)
```

## A. Creamos el mejor modelo lineal multiple con el metodo de los minimos cuadrados

``` r
mod <- lm(tiempo ~ cantidad + distancia, data=softdrink)
```

Al igual que con el modelo lineal simple, utilizamos summary() para ver
los resultados principales del modelo:

``` r
summary(mod)
```

    ## 
    ## Call:
    ## lm(formula = tiempo ~ cantidad + distancia, data = softdrink)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -5.7880 -0.6629  0.4364  1.1566  7.4197 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 2.341231   1.096730   2.135 0.044170 *  
    ## cantidad    1.615907   0.170735   9.464 3.25e-09 ***
    ## distancia   0.014385   0.003613   3.981 0.000631 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 3.259 on 22 degrees of freedom
    ## Multiple R-squared:  0.9596, Adjusted R-squared:  0.9559 
    ## F-statistic: 261.2 on 2 and 22 DF,  p-value: 4.687e-16

## B. Comparacion de modelos: AIC y BIC

Para calcular el AIC y BIC de un modelos, utilizamos las funciones
**AIC()** o **BIC()**

``` r
AIC(mod)
```

    ## [1] 134.8294

``` r
BIC(mod)
```

    ## [1] 139.7049

Sin embargo, no nos brindan mucha información si es que solo tenemos un
modelo. Tenemos que comparar diferentes modelos (que difieran en el
numero de variables independientes).

## C. Intervalos de confianza de los estimadores

Al igual que en la regresion lineal simple, podemos hayar los intervalos
de confianza de los estimadores del modelo.

``` r
confint(mod, level = 0.95)
```

    ##                   2.5 %     97.5 %
    ## (Intercept) 0.066751987 4.61571030
    ## cantidad    1.261824662 1.96998976
    ## distancia   0.006891745 0.02187791

Y también los intervalos de confianza de los valores medios y los
valores predecidos:

## D. Contraste de hipotesis

Como hemos observado, la prueba de ANOVA del modelo, nos permite
determinar si es que el modelo en si, tiene significancia o no (evalua a
todas las variables independientes). Por otro lado, podemos ver
especificamente el efecto de cada una de las variables independientes en
el modelo creado.Ambas evaluaciones se pueden observar mediante el
resumen ofrecido por la función **summary()**

``` r
summary(mod)
```

    ## 
    ## Call:
    ## lm(formula = tiempo ~ cantidad + distancia, data = softdrink)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -5.7880 -0.6629  0.4364  1.1566  7.4197 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 2.341231   1.096730   2.135 0.044170 *  
    ## cantidad    1.615907   0.170735   9.464 3.25e-09 ***
    ## distancia   0.014385   0.003613   3.981 0.000631 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 3.259 on 22 degrees of freedom
    ## Multiple R-squared:  0.9596, Adjusted R-squared:  0.9559 
    ## F-statistic: 261.2 on 2 and 22 DF,  p-value: 4.687e-16

## E. Diagnostico de los residuos

#### E.1. Diagnostico de homocedasticidad de residuos: Test de White

*queda pendiente como realizar este analisis*

Un p-valor pequeño nos indica que tenemos evidencias suficientes para
rechazar que las varianzas de los residuos no son iguales para cualquier
valor de X. Dicho en otras palabras, el modelo no es homocedastico, es
heterocedástico.

#### E.2. Diagnostico de homocedasticidad de residuos: Test de Breusch-Pagan

Realizamos este test con la funcion **bptest()**, pero sin especificar
argumentos.

``` r
library(lmtest)
```

    ## Loading required package: zoo

    ## 
    ## Attaching package: 'zoo'

    ## The following objects are masked from 'package:base':
    ## 
    ##     as.Date, as.Date.numeric

``` r
bptest(mod)
```

    ## 
    ##  studentized Breusch-Pagan test
    ## 
    ## data:  mod
    ## BP = 11.988, df = 2, p-value = 0.002493

Con un p-valor mayor a el nivel de significancia, concluimos
consecuentemente que no tenemos indicios suficientes para rechazar la
homocedasticidad de los errores en este caso.

#### E.3. Diagnostico de normalidad de residuos: Test de Shapiro Wilk + Q-Qplot

Para ellos, aplicamos el test de Shapiro-Wilk a los residuos de nuestro
modelo:

``` r
shapiro.test(mod$residuals)
```

    ## 
    ##  Shapiro-Wilk normality test
    ## 
    ## data:  mod$residuals
    ## W = 0.95151, p-value = 0.2711

En este caso particular, podemos observar que los residuos del modelo se
distribuyen normalmente, ya que el p-valor es mayor al nivel de
significancia y se acepta la Ho. Gráficamente lo podemos observar de la
siguiente manera:

``` r
plot(mod, 2)
```

![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-25-1.png)<!-- -->
Fijenseque a pesar de que existen desviaciones respecto a la
distribución normal, el test de Shapiro-wilk nos ha brindado los
resultados para evidenciar que los residuos se distribuyen normalmente.
Es importante por eso siempre, no solo analizar los gráficos.

#### E.4. Correlacion de residuos: Test de Durbin-Watson

Para este test, utilizaremos la función **dwtest()**:

``` r
library(lmtest)
dwtest(mod, alternative = "greater")
```

    ## 
    ##  Durbin-Watson test
    ## 
    ## data:  mod
    ## DW = 1.1696, p-value = 0.01202
    ## alternative hypothesis: true autocorrelation is greater than 0

En donde el parametro alternative= nos indica si se testea que la
autocorrelación es positiva (greater) o negativa (less). El parametro
que tenemos que brindar a la funcion **dwtest()**, es nuestro modelo.

``` r
dwtest(mod, alternative = "less")
```

    ## 
    ##  Durbin-Watson test
    ## 
    ## data:  mod
    ## DW = 1.1696, p-value = 0.988
    ## alternative hypothesis: true autocorrelation is less than 0

Al obtener un p-valor mayor a alfa, podemos concluir que no hay
autocorrelacion positiva (negativa)en los residuos.

### F. Aditividad y linealidad: Prueba de Tukey

``` r
library(car)
```

    ## Loading required package: carData

    ## 
    ## Attaching package: 'car'

    ## The following object is masked from 'package:dplyr':
    ## 
    ##     recode

    ## The following object is masked from 'package:purrr':
    ## 
    ##     some

``` r
residualPlots(mod, plot=FALSE)
```

    ##            Test stat Pr(>|Test stat|)    
    ## cantidad      2.9807        0.0071298 ** 
    ## distancia     4.4317        0.0002315 ***
    ## Tukey test    3.8494        0.0001184 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

#### F.1. Aditividad y linealidad: Gráfico de los residuos parciales

``` r
library(car)
crPlots(mod)
```

![](7.-Regresión-Lineal_files/figure-gfm/unnamed-chunk-29-1.png)<!-- -->

### G. Analisis de valores atípicos o “outliers” en la regresión lineal

#### G.1. Cálculo de leverages:

#### G.2. Cálculo de outliers

``` r
library(car)
outlierTest(mod)
```

    ##   rstudent unadjusted p-value Bonferroni p
    ## 9  4.31078         0.00030902    0.0077256

En este test, observamos el p-valor, si es menor a nuestro alfa (0.05)
podemos concluir que no existen outliers. Adicionalmente, tambien nos
brinda aquel individuo con el residuo estandarizado mas alto.

#### G.3. Cálculo de observaciones influyentes: Distancia de Cook
